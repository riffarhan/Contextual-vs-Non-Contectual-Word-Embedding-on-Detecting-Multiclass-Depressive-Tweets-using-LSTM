{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riffarhan/Contextual-vs-Non-Contextual-Word-Embedding-on-Detecting-Multiclass-Depressive-Tweets-using-LSTM/blob/main/Word2vec%20%2B%20LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3cekYKSPlon"
      },
      "source": [
        "#Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuARaVH6Prrx"
      },
      "source": [
        "##IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_fYh4jWOCgc",
        "outputId": "1a896dc1-7957-42d9-ff80-6cfd7be77a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip3 install ftfy\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import ftfy\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from math import exp\n",
        "from numpy import sign\n",
        "\n",
        "from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from nltk import PorterStemmer\n",
        "from keras.initializers import Constant\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "#from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, GlobalAveragePooling\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9WRVWs_PzMH"
      },
      "source": [
        "##CONSTANTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7xE_s9IOHLD"
      },
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "np.random.seed(1234)\n",
        "\n",
        "DEPRES_NROWS = 3200  # number of rows to read from DEPRESSIVE_TWEETS_CSV\n",
        "RANDOM_NROWS = 12000 # number of rows to read from RANDOM_TWEETS_CSV\n",
        "MAX_SEQUENCE_LENGTH = 140 # Max tweet size\n",
        "MAX_NB_WORDS = 9000\n",
        "EMBEDDING_DIM = 300\n",
        "TRAIN_SPLIT = 0.6\n",
        "TEST_SPLIT = 0.2\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS= 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfo385rZP5HD"
      },
      "source": [
        "LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJHl2LzJOzax",
        "outputId": "ea371667-3dca-4d66-f48f-fde6c8746347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzDRJBN0OKfu"
      },
      "outputs": [],
      "source": [
        "#FILE PATHS\n",
        "DEPRESSIVE_TWEETS_CSV = '/content/drive/MyDrive/Colab Notebooks/DATA SOURCE 1/dataset/depressive_tweets_processed.csv'\n",
        "RANDOM_TWEETS_CSV = '/content/drive/MyDrive/Colab Notebooks/DATA SOURCE 1/dataset/Sentiment Analysis Dataset.csv'\n",
        "EMBEDDING_FILE = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PVzc4hu6nPz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/dataset/archive 2/DatasetBaru.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtCoTJbCPeBJ"
      },
      "outputs": [],
      "source": [
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_k5Jb2PPZrk"
      },
      "outputs": [],
      "source": [
        "df['Sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCejMHNTQIG1"
      },
      "source": [
        "#Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdz9WzDrQRQX"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka0MMwo3QWCn"
      },
      "source": [
        "Preprocessing the tweets in order to:\n",
        "\n",
        "\n",
        "\n",
        "*   Remove links and images\n",
        "*   Remove hashtags\n",
        "*   Remove @ mentions\n",
        "*Remove emojis\n",
        "*Remove stop words\n",
        "*Remove punctuation\n",
        "*Get rid of stuff like \"what's\" and making it \"what is'\n",
        "*Stem words so they are all the same tense (e.g. ran -> run)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnRBZtcoQOJQ"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction\n",
        "cList = {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"I'd\": \"I would\",\n",
        "  \"I'd've\": \"I would have\",\n",
        "  \"I'll\": \"I will\",\n",
        "  \"I'll've\": \"I will have\",\n",
        "  \"I'm\": \"I am\",\n",
        "  \"I've\": \"I have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it had\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so is\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there had\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we had\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'alls\": \"you alls\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you had\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you you will\",\n",
        "  \"you'll've\": \"you you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
        "\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return cList[match.group(0)]\n",
        "    return c_re.sub(replace, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN8IAcR3QSS8"
      },
      "outputs": [],
      "source": [
        "def clean_tweets(tweets, sentiments):\n",
        "    cleaned_tweets = []\n",
        "    cleaned_sentiments = [] \n",
        "    for i, tweet in enumerate(tweets):\n",
        "        tweet = str(tweet)\n",
        "        # if url links then dont append to avoid news articles\n",
        "        # also check tweet length, save those > 10 (length of word \"depression\")\n",
        "        if re.match(\"(\\w+:\\/\\/\\S+)\", tweet) == None and len(tweet) > 10:\n",
        "            #remove hashtag, @mention, emoji and image URLs\n",
        "            tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(\\#[A-Za-z0-9]+)|(<Emoji:.>)|(pic\\.twitter\\.com\\/.)\", \" \", tweet).split())\n",
        "            \n",
        "            #fix weirdly encoded texts\n",
        "            tweet = ftfy.fix_text(tweet)\n",
        "            \n",
        "            #expand contraction\n",
        "            tweet = expandContractions(tweet)\n",
        "\n",
        "            #remove punctuation\n",
        "            tweet = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", tweet).split())\n",
        "\n",
        "            #stop words\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            word_tokens = nltk.word_tokenize(tweet) \n",
        "            filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "            tweet = ' '.join(filtered_sentence)\n",
        "\n",
        "            #stemming words\n",
        "            tweet = PorterStemmer().stem(tweet)\n",
        "            \n",
        "            cleaned_tweets.append(tweet)\n",
        "            cleaned_sentiments.append(sentiments[i])\n",
        "\n",
        "    return cleaned_tweets, cleaned_sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrpYV2CoKg8n"
      },
      "outputs": [],
      "source": [
        "# merubah type data dalam record menjadi format string\n",
        "df.SentimentText=df.SentimentText.astype(str)\n",
        "#@title\n",
        "# merubah type data dalam record sentimentText menjadi format string\n",
        "#depressive_tweets_df=depressive_tweets_df.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqf93YHibafP"
      },
      "outputs": [],
      "source": [
        "#depressive_tweets_arr = [x for x in depressive_tweets_df[5]]\n",
        "random_tweets_arr = [x for x in df['SentimentText']]\n",
        "#X_d = clean_tweets(depressive_tweets_arr)\n",
        "X_r, y_r = clean_tweets(random_tweets_arr, df['Sentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCQIEYgQgC_2"
      },
      "source": [
        "##Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaYhXyXzgEmt"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(X_r)\n",
        "sequences_r = tokenizer.texts_to_sequences(X_r)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens' % len(word_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Padding"
      ],
      "metadata": {
        "id": "w-487fuCmkym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsoXAi7XgKwY"
      },
      "outputs": [],
      "source": [
        "data_r = pad_sequences(sequences_r, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data_r tensor:', data_r.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Embedding"
      ],
      "metadata": {
        "id": "9nHIGndwmqHO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slkt_mNLzoOX"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_FILE = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin'\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "\n",
        "for (word, idx) in word_index.items():\n",
        "    if word in word2vec.vocab and idx < MAX_NB_WORDS:\n",
        "        embedding_matrix[idx] = word2vec.word_vec(word)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "print(a)\n",
        "print(EMBEDDING_DIM)\n",
        "print(nb_words)"
      ],
      "metadata": {
        "id": "MPnqAcciSSax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg9FzJybI9x8"
      },
      "outputs": [],
      "source": [
        "nb_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Splitting & One Hot Encoding"
      ],
      "metadata": {
        "id": "AVsl4xuvF0dW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcWvlpznGBzH"
      },
      "outputs": [],
      "source": [
        "# defining feature matrix(X) and response vector(y)\n",
        "X = data_r\n",
        "y = y_r\n",
        "y = np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKwIJQzyAxyo"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "y = pd.get_dummies(y)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8jCyLJ5zrt8"
      },
      "outputs": [],
      "source": [
        "# splitting X and y into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-OHxQcf-n-w"
      },
      "source": [
        "##Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5SaR_51TcbQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(embedding_matrix), \n",
        "                               output_dim=EMBEDDING_DIM, \n",
        "                               input_length=MAX_SEQUENCE_LENGTH, \n",
        "                               embeddings_initializer=Constant(embedding_matrix)),\n",
        "    # tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.LSTM(300, return_sequences = True),\n",
        "    # tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(150, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTXFN1BI0bEl"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYFlk3OY0-Mt"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/Checkpoint Farhan/\"\n",
        "callbacks_list = [ModelCheckpoint(checkpoint_path+\"New_Word2vec + Regularization\",\n",
        "                                  monitor='val_acc',\n",
        "                                  save_best_only=True,\n",
        "                                  mode='max',\n",
        "                                  verbose=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrhRe-51-O7s"
      },
      "outputs": [],
      "source": [
        "print(tf.keras.utils.plot_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSab--AD0eAs"
      },
      "outputs": [],
      "source": [
        "early_stop = EarlyStopping(monitor='val_acc', patience=5) #Stop training when a monitored metric has stopped improving.\n",
        "\n",
        "hist = model.fit(X_train, y_train, \\\n",
        "        validation_data=(X_test, y_test), \\\n",
        "        epochs=10, batch_size=16, shuffle=True, \n",
        "        callbacks = early_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 3"
      ],
      "metadata": {
        "id": "cRcXebJOnJSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Accuracy & Loss"
      ],
      "metadata": {
        "id": "WkCw97-ynVmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FKtjgjR0493"
      },
      "outputs": [],
      "source": [
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAcTH3TQ0ipR"
      },
      "outputs": [],
      "source": [
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocVI8stJ0S1x"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Confusion Matrix"
      ],
      "metadata": {
        "id": "wfpchJ7Hnd3I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbBzC-MCz83E"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = model.predict(X_test)\n",
        "report_dict = classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1), digits=3, output_dict = True)\n",
        "pd.DataFrame(report_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg_KAAk3bYUV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = model.predict(X_test)\n",
        "plt.figure(3)\n",
        "cf_matrix = ConfusionMatrixDisplay.from_predictions(y_test.argmax(axis=1), y_pred.argmax(axis=1), cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "BCQIEYgQgC_2"
      ],
      "machine_shape": "hm",
      "name": "Word2vec + LSTM",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}